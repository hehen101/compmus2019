---
title: "Throwbacktime"
author: "by Helen H."
date: "03.2019"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: flatly
---

```{r, cache = FALSE}
devtools::install_github('charlie86/spotifyr')
devtools::install_github('jaburgoyne/compmus')
library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)
library(flexdashboard)
library(rsample)
library(recipes)
library(parsnip)
library(yardstick)
library(ggdendro)
library(protoclust)
source('Spotify.R')
```

Introduction
=========================================

## Section 1

Have you ever listened to a song and you knew exactly in what kind of situation you listenend to it before? Did you ever get the feeling that a certain song was connected to a special moment and if you listen to it you could immediately tell when it was? Did you ever wonder how an events may have influenced your music taste?
I started this project with thinking about what I am **interested in and what kind of music I listen to the most**. So I decided to give my project a rather personal touch by using my own playlists. I have been using Spotify for years now, listening to music for several hours every day; I love the variety of music, the ability of **creating your own playlists**, the premium account with no ads, better sound quality and of course the ability to download playlists which allows me to listen to my playlists anytime anywhere.
I made my first own playlist in January 2014 and ever since continued to make in **irregular intervals** new ones, containing new but sometimes also songs which I had listened to in previous playlists. After a while, when I didn't like the music anymore or got bored by it I would make a new one. Especially in the last two to three years I started using Spotify on a daily basis, listening to music on my phone and laptop, sometimes just as a quiet background music, however more often especially for the purpose of enjoying it, which is the reason why I have chosen playlists from 2016 and 2018. With some songs I have the sensation they are somehow connected to one special moment, one special feeling. In particular in the last few mentioned years I experienced a lot of unique events which is why decided to have a closer look at them. With this project, I will try to see if and how my music taste has changed, keeping in mind which events I connect to certain songs.

## Section 2
```{r pressure, echo=FALSE, out.width = '75%'}
knitr::include_graphics("Collage.png")
```

# Content 
##Section 1
### Songs in 2016
```{r}
valueBox(value = "508", caption = "Songs in 2016", icon = "fa-headphones")
```

### 2016

The playlist contains mostly songs which were **popular** back in that year. It also contains a lot of **German music** because I went to a small festival organized by a German band ("Kraftklub") with a lot of German artists, for example "Casper", "Prinz Pi" and "Frittenbude". Furthermore, there are quite a lot of **rock songs** since I went to another festival which is called "Rock am Ring". As the name already suggests most of the artists are from the rock genre, like "Bring me the Horizon", "Volbeat" and "Billy Talent", but there are also a few german rappers, for example "Alligatoah", and pop bands such as "Major Lazor" or "Panic! at the Disco". Before I went to the festivals mentioned above I listened to playlists I made containing all the artists that I liked and wanted to see. Lastly, I added throughout the year some songs **Spotify would recommend** to me or songs that I found "by accident" for example by clicking on a random song or songs people would tell me about.

## Section 2
### Songs in 2018
```{r}
valueBox(397, caption = "Songs in 2018", icon = "fa-headphones")
```

### 2018

For the playlists in 2018 I used the same methods. Again I went to "Rock am Ring", this time big **rock bands** like "Rise Against", "Foo Fighters" and "Muse" made it into various playlist but also other bands from the festival like "Walking on Cars" and the **German rapper** "Casper" got added. Additionally, some more soft artists, more or less from the **indiegenre** and some more rock bands catched my eye. My music taste varied a lot last year - I had some months were I would only listen to music one could describe as soft and girly-like whereas in other months most of the songs were made by rock and alternative bands like "Starset" and "Mayday Parade".

2016 vs. 2018 - what has changed? {.storyboard}
=========================================

### First comparison - has anything changed?

```{r}

zweisechs <- get_playlist_audio_features('helenhense','04p2OmKrUTiuDEiHZ1kTwa')
zweiacht <- get_playlist_audio_features('helenhense','2ghNE1Ph4QjLdFwaKMgam5')

zusammen <- 
  zweisechs %>% mutate(playlist = "2016") %>%
  bind_rows(zweiacht %>% mutate(playlist = "2018"))

vergleich <- 
  zusammen %>%
  ggplot(aes(x = danceability, y = energy, size = acousticness, color = loudness, label2 = artist_name, label = track_name)) + 
  geom_point(alpha = 0.5)  +
  facet_wrap(~ playlist) +
  scale_x_continuous(         
      limits = c(0, 1),
      breaks = c(0, 0.25, 0.50, 0.75, 1),  
      minor_breaks = NULL
      ) +
  scale_y_continuous(          
    limits = c(0, 1),
    breaks = c(0, 0.25, 0.50, 0.75, 1),
    minor_breaks = NULL
    ) +
  theme_linedraw() +
  labs( 
    x = "Danceability",
    y = "Energy"
  ) 
ggplotly(vergleich)
```

--------------------------

Let's have a first look at the plots for each year. On the **x-asis the danceability** is shown, wheres on the **y-asis the energy** is pictured. Furthermore, the **size** gives information about the **acousticness** and the **color about loudness**.
What is common in these two visualizations? First of all, it is very clear that songs with a high energy are less acoustic, which makes sense since acoustic songs tend to be more softer and calm. Furthermore, both years cover a lot of the danceability scale with most songs being in between 0.25 and 0.8. Even though both look quite similar, there are some differences. Way more songs had a **lower energy** in 2018, whereas more than 75% of songs in 2016 are above 0.5. There are a lot of songs under 0.25 energy in 2018, but in 2016 there are only two. Additionally the songs from 2016 are mostly crowded at a **very high energy** above 0.75, wheres in 2018 most of them range between 0.5 and 1. This makes a lot of sense because I listened to a lot of sad music by end of 2018 which was the time my (ex-)boyfriend and I split up. Moreover, looking at the highest and lowest measurement, it is clear that there is a big gap between them within both years. Let us have a closer look at an example: The lowest measurement for danceability in 2016 got assigned to  “Happy Endings Are Stories That Haven’t Ended Yet” by “Mayday Parade” with a value of 0,165 , the highest to “Emanuela” by “Fettes Brot” with 0,909.  In 2018 the lowest measurement was found for “Look Up and See Infinity, Look Down and See Nothing” by “Mayday Parade” and the highest for “Bilder mit Katze” by “Frittenbude” with 0,917. I found it really interesting that “Mayday Parade” showed up twice for the lowest measurement and in fact will accure several times for other features as well. The summarize revealed the maximum at 0,565 and the minimum at 0,155 for 2016, for 2018 it was 0,562 for the maximum and 0,139 for the minimum.

It is very clear that both, **danceability and energy, sunk within two years**. The data provides an indication of slightly shifts in musical taste over the years. In my opinion the danceability sunk because the 2018 playlist contains many rock songs, whereas the 2016 contains more popsongs. It was a surprise though that the energy sank, too. I would have suggested that it would rise since in my opinion rock songs are more energetic and louder than pop- or indiesongs.

### Clustering January 2016
```{r}
halloween <- get_playlist_audio_features('helenhense', '6zoPyMRYE1cSPY2N3ICrWX') %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)
```

```{r}
halloween_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = halloween) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(halloween %>% mutate(track_name = str_trunc(track_name, 20))) %>% 
    juice %>% 
    column_to_rownames('track_name')
halloween_dist <- dist(halloween_juice, method = 'euclidean')
hclust(halloween_dist, method = 'single') %>% dendro_data %>% ggdendrogram
```

------------------------

This visualization provides us information about the clustering of the playlist I made in January 2016. I have chosen this playlist because it is the first one I made within the defined period.  
The very first thing that is clearly visible is the clustering of the two songs "Catch & Release" and "Lost out Here". Those two songs are mostly by **themselves** whereas most of the other songs somehow belong to a bigger group. Another interesting detail is the fact that the first six songs on the left are by themselves and only get paired up with other songs while going down the **hierarchy**, which is in general a **very high** one. This means that most songs are paired up and the lowest point in the hierarchy are always sets of two. These sets get than either teamed up with another set of two or single songs until we reach about two-thirds of the whole clustering. 
Let's have a closer look at the songs themselves: 
On the left we have the single songs like "Love yourself" which is has low energy but rather high acousticness. The second song on the left is "The Horns" which has a very low acousticness and speechiness, but a high rate of danceability and energy. I find this **pairing rather odd**, since both songs are very different and do not have anything in common. On the very far right side "Lost out Here" ranges for most of the values somewhere in the middle or a little bit over that except for instrumentalness, whereas "Catch & Release" is very low in speechiness and instrumentalness, in the middle for energy and acousticness and rather high in danceability. Both songs belong somewhat to techno but also pop genre. 
In my opinion the clustering worked at least in some parts for the playlist, but there are some odd pairs which I would not have expected. Furthermore, the hierarchy displays how many different genres are within this playlist, all of the songs are clustered within really small groups and there are no big chunks. 

### Clustering November 2018 

```{r}
halloween <- get_playlist_audio_features('helenhense', '1SZbj2UvLMD0OKFTQgySpE') %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)
```

```{r}
halloween_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = halloween) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(halloween %>% mutate(track_name = str_trunc(track_name, 20))) %>% 
    juice %>% 
    column_to_rownames('track_name')
halloween_dist <- dist(halloween_juice, method = 'euclidean')
hclust(halloween_dist, method = 'single') %>% dendro_data %>% ggdendrogram
```

---------------------------------
The clustering for **November 2018** (the last playlist I made that year) looks at a first glance quite similar to the one for 2016, but there are **small differences**. The songs on the left side are again by themselves until the fifth song. Afterwards some songs get paired into small clusters of either two or single songs which are **all connected to the first songs**. “Into waves” is the song on the far left, with a high danceability (0.853) but rather low speechiness (0.0439) and acousticness (0.148) which makes at least partly sense because it is a very popular dance track. I am a bit surprised by the low speechiness since the song contains actually quite a lot of singing, but also uses a lot of autotune and voice changer. Next to that song we can find “Nobody” which has similar values for danceability and energy. Let’s move to the right side: The very last pair is “The Book of Love” and “EVERY SINGLE DAY” - two songs I would describe as sad and slow. Those two songs got clustered with “little light” in the acoustic version and “The Story Never Ends”, again two songs which are sad and slow. “Little light” is played on a guitar, whereas “The Story Never Ends” is played mostly on a piano, both with a high acousticness, which explains the pairing of these two. 
Upon a closer reconsideration the clustering for the playlist makes more sense – the sad and slow songs are clustered to the left, with a rising energy to the right. It seems as if the songs in this playlist seem to have more in common, wheres as in the clustering for 2016 many songs were not paired up to some other songs and were by themselves. 

### Chroma & Timbrefeatures 

```{r}
lookup <- 
    get_tidy_audio_analysis('5wA0nTnWYZb97FeQRuf2QQ') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))
lookupplot <- 
    bind_rows(
        lookup %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        lookup %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')
lookupplot
```

----------------------------
In these two visualization the song "Blau" by Kraftklub was analysised with the Spotidy API. It is one of the songs with the highest energy and danceability, wheres the acousticness is rather low. What we can easily see within both visualizations is that the Spotify API picks up the chorus really good; especially in the timbre feature: The "main" chorus is represented with the dark blue boxes (e.g. 0:50 until 1:20 min), whereas the "intro" for the chorus is shown with small dark blue boxes such as at about 1:50 min into the song. Another observation which is very clear is the beginning before the singer starts which can also be seen very clearly within in the timbre plot. At about 2:30 it gets very quite with just some drums and the singer but the tension gets built up into a climax at 2:55 which is represented by the bright yellow line and can be seen in both visualizations. Afterwards the chorus starts again until the song comes to a sudden end. 
For the chroma visualization there is not much change trough out the whole song except for the sudden key change which I already mentioned above. The repetition lines can be barely seen and are most clear in between 1:40 until 2:30 where the drums start. 

I liked this song for the representation because both visualizations looked quite similar which I thought was unusual. I expected to have some more variety within them. 

### Tonalanalysis

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
alles_ist_erleuchtet <- 
    get_tidy_audio_analysis('5x4ugJzEZAPn9nbbD9xKJV') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (sec)', y = '', fill = 'Distance')
ggplotly(alles_ist_erleuchtet)
```

-------------------------------

Within this keygram for the tonal analysis of the song "Holland" by "257ers" we can see that the song **starts with E min. and switches to C# min** after about 15 seconds. The key stays there for more or less the whole song even though it is blurry and not constant, which is indicated by the fading dark blue colour into a more yellowish line. At the very end B min is the darkest but all of the other keys are very dark, too, hence the Spotify API failed to analyze the end correctly. The high usage of C# makes sense, since it is the most convenient key to play on a keyboard, which is the main instrument within the song. 
The song starts with a solo playing keyboard until the singing starts and the key change can be seen in the keygram and can be also heard. "Holland" has a rather high value within energy and danceability. 


### Classification



# Any results? 
### Conclusion

Within this project I tried to find out if my music taste has **changed through the last two years**. Because unique events such as a break up and going abroad to China happened during this years I wondered if it had **any influence** on my music. 
Already within the first visualization it was clear that there has been **some change** even though it is **tiny**. 
Since both playlists contained various artists and bands from all over the world and multiple genres such as rock, happy, techno but also melancholic, it was **nearly impossible** to find pieces which would **represented the whole playlist well and in a good way**. In retrospect I am still glad I have chosen this topic though. I expected that there would be more change because of the two years time difference and unusual events but surpringsly the changes have been small. What we can take from this analysis is that the music got somewhat less "happy" and turned more into the melancholic area, less energy, less danceability and less loudness, which clearly shows that my depression within that year got worse. 2018 has been a rough year for me and it is no surprise that this is represented in the music I listen to. Furthermore, I discovered that there are approximately 100 songs more in the 2016 playlist than the 2018 which really surprised me. Usually my playlists contain about 40-60 songs for each month, but the september 2018 playlist contained more than 200 because I made it for the flight to China. It contained many songs which I usually would not put in a daily playlist but to avoid repeating songs during that long flight, I did. That is the reason why I am surprised about the small amount of songs I had in the other playlists, indicating that I listened to mostly the same songs through out the whole year.  
**All in all**, I am partly surprised about the just little change, on the other side it makes sense since in my opinion ones music taste does not change completly within a two-year period and I have enjoyed listening to rock and melancholic music for before 2016, too. In fact, I do not listen to more rock music now, but rather listen to less pop and charts than I did when I was younger, focussing now on music with a message and more instruments and less autotune. 

Appendix {.storyboard}
=========================================

### Unusual keygram I found 

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))
alles_ist_erleuchtet <- 
    get_tidy_audio_analysis('7klmc0PWfPlMJgpiHklzT2') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    compmus_match_pitch_template(key_templates, 'aitchison', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (sec)', y = '', fill = 'Distance')
ggplotly(alles_ist_erleuchtet)
``` 

--------------------------------------------
This keygram catched my eye because the Spotify API did an awful job analyzing this song which is called "Lass liegen" by "Alligatoah", a german rapper, who is sociocritical and addresses many issues in a sarcastic way. In that particiular song he sings about the throwaway society human kind are becoming. The song starts with just the singer and a simple beat on the piano which is represented by a very blurry mix of blue and yellow and turns at about 25 seconds into the song, where the rest of the instruments start playing. The rest of the visualization is then dark blue with very little changes and the end is represented with a sudden change to bright yellow. 

### Links and information about everything I used
Pictures used for the coverpicture:

| Name of artist/band       | Link |
|-------------|:----------------------------------------:|
| Cro |     https://www.redbull.com/de-de/cro-melodie-im-chec  |
| Mayday Parade      |      https://en.wikipedia.org/wiki/Mayday_Parade |
| Emancipator   |  https://www.melkweg.nl/nl/agenda/emancipator-ensemble-05-11-2018/ |
| Walking on cars  |      https://de.wikipedia.org/wiki/Walking_on_Cars |
| Alligatoah       |  https://deutscherrap.fandom.com/de/wiki/Alligatoah |
| Breaking Benjamin     | https://www.imdb.com/name/nm1736151/ |
| Casper | http://www.danielwagner.photo/portfolio/casper/ | 
| Muse | https://www.laut.de/Muse | 
| Dean Lewis | https://www.amazon.de/Dean-Lewis/e/B072N3VX41 | 


Furthermore I used Spotify and the Spotify API.



